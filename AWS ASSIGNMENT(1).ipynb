{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116d6a08-c57c-44fc-b60d-730da784d4fe",
   "metadata": {},
   "source": [
    "# AWS ASSIGNMENT>>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2135b6-276c-4d73-8e1a-cbf35d45f82e",
   "metadata": {},
   "source": [
    "# 1.Explain the difference between AWS Regions, Availability Zones, and Edge Locations. Why is this important for\n",
    "# data analysis and latency-sensitive applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e052c-6816-4a5d-be7d-4512fecf7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS Region:\n",
    "A Region is a geographical area (like us-east-1, eu-west-1) containing multiple isolated data centers known as Availability Zones.\n",
    "\n",
    "Each region is physically separated from others to ensure fault tolerance.\n",
    "\n",
    "Availability Zone (AZ):\n",
    "An Availability Zone is one or more discrete data centers within a region, each with redundant power, networking, and connectivity.\n",
    "\n",
    "They are close enough for low-latency replication but isolated enough to protect applications from failures in other zones.\n",
    "\n",
    "Edge Location:\n",
    "Edge Locations are data centers that AWS uses to cache content closer to end-users.\n",
    "\n",
    "Mainly used by services like Amazon CloudFront and AWS Global Accelerator to reduce latency in content delivery.\n",
    "Fault Tolerance & High Availability: Deploying across multiple AZs or Regions ensures your data analytics apps stay up even if one data center fails.\n",
    "\n",
    "Low Latency: Placing compute or caching layers (like CloudFront) near users or data sources improves real-time analytics performance.\n",
    "\n",
    "Compliance: Some data must stay within specific countries or regions due to legal requirements.\n",
    "\n",
    "Global Reach: You can deploy services closer to global users for better responsiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4269e-0c7c-433d-bf10-3477681187e2",
   "metadata": {},
   "source": [
    "# 2.Using the AWS CLI, list all available AWS regions. Share the command used and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbcce2-cf4c-4dd0-acbc-cc889761ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws ec2 describe-regions --all-regions --query \"Regions[*].RegionName\" --output table\n",
    "----------------------------------\n",
    "|        DescribeRegions         |\n",
    "+--------------------------------+\n",
    "|  af-south-1                   |\n",
    "|  ap-east-1                    |\n",
    "|  ap-south-1                   |\n",
    "|  ap-northeast-1               |\n",
    "|  ap-northeast-2               |\n",
    "|  ap-northeast-3               |\n",
    "|  ap-southeast-1               |\n",
    "|  ap-southeast-2               |\n",
    "|  ca-central-1                 |\n",
    "|  eu-central-1                 |\n",
    "|  eu-west-1                    |\n",
    "|  eu-west-2                    |\n",
    "|  eu-west-3                    |\n",
    "|  eu-north-1                   |\n",
    "|  eu-south-1                   |\n",
    "|  me-south-1                   |\n",
    "|  sa-east-1                    |\n",
    "|  us-east-1                    |\n",
    "|  us-east-2                    |\n",
    "|  us-west-1                    |\n",
    "|  us-west-2                    |\n",
    "+--------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e64c9-1183-4042-965a-7723982c93d9",
   "metadata": {},
   "source": [
    "# 3.Create a new IAM user with least privilege access to Amazon S3. Share your attached policies (JSON or\n",
    "screenshot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b27ad-372a-428f-bee6-b9498baad714",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws iam create-user --user-name s3-readonly-user\n",
    "\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:ListBucket\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:s3:::my-analytics-bucket\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:GetObject\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:s3:::my-analytics-bucket/*\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "aws iam put-user-policy \\\n",
    "  --user-name s3-readonly-user \\\n",
    "  --policy-name S3ReadOnlyPolicy \\\n",
    "  --policy-document file://s3_readonly_policy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53295808-fcdb-4ec4-93b1-ed57f13ebc9d",
   "metadata": {},
   "source": [
    "# 4. Compare different Amazon S3 storage (Standard, Intelligent-Tiering, Glacier). When should each be used in data analytics workflows?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa5b68-ea9e-4e60-8f66-fbeff9e8e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "| **Storage Class**       | **Use Case**                                                    | **Key Features**                                          |\n",
    "| ----------------------- | --------------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| S3 Standard             | Frequently accessed data (active datasets)                      | Low latency, high throughput                              |\n",
    "| S3 Intelligent-Tiering  | Unpredictable access patterns                                   | Automatic tiering; cost-optimized without performance hit |\n",
    "| S3 Standard-IA          | Infrequently accessed data with quick retrieval needs           | Cheaper than Standard; charges per retrieval              |\n",
    "| S3 One Zone-IA          | Infrequently accessed, non-critical data (can tolerate AZ loss) | Cheaper but only stored in one AZ                         |\n",
    "| S3 Glacier              | Archived data, accessed occasionally (minutes to hours)         | Low cost, retrieval time in minutes/hours                 |\n",
    "| S3 Glacier Deep Archive | Rarely accessed archives (e.g., compliance backups)             | Lowest cost, retrieval takes up to 12 hours               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5005d-1a1d-4152-83a1-2df6551dd535",
   "metadata": {},
   "source": [
    "# 5. Create an S3 bucket and upload a sample dataset (CSV or JSON). Enable versioning and show at least two versions of one file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f1e5e-accf-42e6-9092-6aef825e330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket\n",
    "aws s3api create-bucket --bucket analytics-sample-data --region us-east-1\n",
    "\n",
    "# Enable versioning\n",
    "aws s3api put-bucket-versioning --bucket analytics-sample-data --versioning-configuration Status=Enabled\n",
    "\n",
    "# Upload v1 of file\n",
    "aws s3 cp sample_data.csv s3://analytics-sample-data/sample_data.csv\n",
    "\n",
    "# Modify file locally, then re-upload (v2)\n",
    "echo \"new,data,line\" >> sample_data.csv\n",
    "aws s3 cp sample_data.csv s3://analytics-sample-data/sample_data.csv\n",
    "\n",
    "# List object versions\n",
    "aws s3api list-object-versions --bucket analytics-sample-data --prefix sample_data.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939691e-f1c5-45f1-8339-7a6c6d1740c7",
   "metadata": {},
   "source": [
    "# 6.Write and apply a lifecycle policy to move files to Glacier after 30 days and delete them after 90. Share the policy JSON or Screenshot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20572966-d63f-47e2-8202-5636667a4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"Rules\": [\n",
    "    {\n",
    "      \"ID\": \"MoveToGlacierAndDelete\",\n",
    "      \"Status\": \"Enabled\",\n",
    "      \"Prefix\": \"\",\n",
    "      \"Transitions\": [\n",
    "        {\n",
    "          \"Days\": 30,\n",
    "          \"StorageClass\": \"GLACIER\"\n",
    "        }\n",
    "      ],\n",
    "      \"Expiration\": {\n",
    "        \"Days\": 90\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "aws s3api put-bucket-lifecycle-configuration \\\n",
    "  --bucket analytics-sample-data \\\n",
    "  --lifecycle-configuration file://lifecycle.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24763400-5af9-4905-8320-2b87ac64e464",
   "metadata": {},
   "source": [
    "# 7.Compare RDS, DynamoDB, and Redshift for use in different stages of a data pipeline. Give one use case for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b174c4f-eb83-42cd-8cb1-c64b90d1bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "| **Service**  | **Best For**                                 | **Use Case**                                     |\n",
    "| ------------ | -------------------------------------------- | ------------------------------------------------ |\n",
    "| **RDS**      | Relational data, transactions                | Storing normalized app data (e.g., sales orders) |\n",
    "| **DynamoDB** | Key-value or document-based, low-latency ops | Real-time leaderboard, IoT device states         |\n",
    "| **Redshift** | Columnar, petabyte-scale analytics           | Running SQL queries on massive datasets          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1c851-f699-4f35-9bcf-4fe92470c7e2",
   "metadata": {},
   "source": [
    "# 8.Create a DynamoDB table and insert 3 records manually. Then write a Lambda function that adds records when triggered by S3 uploads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6b3f7-fd42-476a-ad82-a028296b7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws dynamodb create-table \\\n",
    "  --table-name UploadEvents \\\n",
    "  --attribute-definitions AttributeName=FileName,AttributeType=S \\\n",
    "  --key-schema AttributeName=FileName,KeyType=HASH \\\n",
    "  --billing-mode PAY_PER_REQUEST\n",
    "\n",
    "aws dynamodb put-item --table-name UploadEvents --item '{\"FileName\": {\"S\": \"file1.csv\"}}'\n",
    "# Repeat for file2.csv to file5.csv\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for record in event['Records']:\n",
    "        key = record['s3']['object']['key']\n",
    "        dynamodb.put_item(\n",
    "            TableName='UploadEvents',\n",
    "            Item={'FileName': {'S': key}}\n",
    "        )\n",
    "    return {'statusCode': 200}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79befade-7190-4c55-a95c-a5a256cfa7e1",
   "metadata": {},
   "source": [
    "# 9.What is serverless computing? Discuss pros and cons of using AWS Lambda for data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dff60a-c5a6-4ec2-8d96-c4f4bc632d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition:\n",
    "Serverless computing means running code without managing servers — you only pay for what you use.\n",
    "\n",
    "#Pros:\n",
    "Auto scaling\n",
    "\n",
    "No server management\n",
    "\n",
    "Cost-effective for infrequent jobs\n",
    "\n",
    "#Cons:\n",
    "Timeout limits (15 minutes for Lambda)\n",
    "\n",
    "Cold start latency\n",
    "\n",
    "Complex workflows require orchestration (e.g., Step Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e504a4-cbee-4f16-aa20-a06d06e12143",
   "metadata": {},
   "source": [
    "# 10.Create a Lambda function triggered by S3 uploads that logs file name, size, and timestamp to Cloudwatch. Share code and a log screenshot?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547feee-ce6b-4d59-a644-6af7ee2d3940",
   "metadata": {},
   "source": [
    "import logging\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for record in event['Records']:\n",
    "        key = record['s3']['object']['key']\n",
    "        size = record['s3']['object']['size']\n",
    "        time = record['eventTime']\n",
    "        logging.info(f\"File uploaded: {key}, Size: {size}, Time: {time}\")\n",
    "    return {'statusCode': 200}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f1cb4-3bc7-461d-ba6e-efed975ffb9c",
   "metadata": {},
   "source": [
    "# 11.Use AWS Glue to crawl your S3 dataset, create a Data Catalog table, and run a Glue job to convert CSV data to parquet. Share job code and output location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d5b8d-26df-4cf9-b560-0913c3956916",
   "metadata": {},
   "source": [
    "We will:\n",
    "\n",
    "Set up an S3 bucket and upload CSV data\n",
    "\n",
    "Create a Glue Crawler to detect schema and create a Data Catalog table\n",
    "\n",
    "Write a Glue Job (Spark script) to convert CSV to Parquet\n",
    "\n",
    "Output the Parquet files to a new S3 location\n",
    "\n",
    "aws s3 cp sample_data.csv s3://analytics-sample-data/raw/\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Boilerplate\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Load CSV table from Glue Catalog\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"analytics_db\", \n",
    "    table_name=\"raw_sample_data\", \n",
    "    transformation_ctx=\"datasource\"\n",
    ")\n",
    "\n",
    "# Write to Parquet format\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=datasource,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://analytics-sample-data/parquet/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "job.commit()\n",
    "\n",
    "Replace \"raw_sample_data\" with the actual table name the crawler generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24305994-0093-47cd-9055-eb813c7bf5f3",
   "metadata": {},
   "source": [
    "# 12.Explain the difference between Kinesis Data Streams, Kinesis Firehose, and Kinesis Data Analytics. Provide a\n",
    "real-world example of how each would be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f44cd-0e9f-4bfe-8de1-7ae9dd5bb78a",
   "metadata": {},
   "source": [
    "#1.Kinesis Data Streams (KDS)\n",
    "What it is: A real-time data streaming service that lets you collect, process, and store large streams of data records (millisecond latency).\n",
    "\n",
    "#Use case: Best for custom processing logic using AWS Lambda, EC2, or Kinesis Client Library.\n",
    "\n",
    "Example:\n",
    "A ride-hailing app streams GPS data from drivers to Kinesis Data Streams, which triggers a Lambda to calculate traffic heatmaps in real-time.\n",
    "\n",
    "#2.Kinesis Data Firehose\n",
    "What it is: A fully managed data delivery service that automatically loads streaming data to destinations like S3, Redshift, or Elasticsearch.\n",
    "\n",
    "No manual provisioning needed.\n",
    "\n",
    "Near real-time, but with buffering (up to 60 secs or 1MB).\n",
    "\n",
    "Example:\n",
    "A financial app sends transaction logs to Kinesis Firehose, which automatically stores the data in S3 in Parquet format, then loads it into Redshift for reporting.\n",
    "\n",
    "#3. Kinesis Data Analytics\n",
    "What it is: A service to run SQL or Apache Flink on streaming data in real time (from Kinesis Data Streams or Firehose).\n",
    "\n",
    "Great for real-time filtering, aggregations, or joins without building code.\n",
    "\n",
    "Example:\n",
    "A logistics company uses Kinesis Data Analytics to detect temperature anomalies from IoT sensor data streaming in via Data Streams, triggering alerts when thresholds are exceeded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1e59e-7d8a-4f14-9772-3adf08036e02",
   "metadata": {},
   "source": [
    "# 13.What is columnar storage and how does it benefit Redshift performance for analytics workloads\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d5e8f-b41d-47f7-a859-8eff941f5426",
   "metadata": {},
   "source": [
    "How Columnar Storage Works in Redshift:\n",
    "Redshift stores data by columns.\n",
    "\n",
    "Instead of reading an entire row, Redshift reads only the relevant columns needed for a query.\n",
    "\n",
    "This reduces the amount of data that needs to be scanned, improving I/O efficiency.\n",
    "\n",
    "Benefits of Columnar Storage for Redshift Analytics Workloads:\n",
    "Efficient Data Scanning:\n",
    "\n",
    "Analytics queries often access only a subset of columns.\n",
    "\n",
    "Columnar storage means Redshift reads only those columns, reducing disk I/O.\n",
    "\n",
    "Better Compression:\n",
    "\n",
    "Data within a column tends to be of the same data type and often has similar values.\n",
    "\n",
    "This makes compression more effective, reducing storage size and improving query speed.\n",
    "\n",
    "Faster Aggregations and Computations:\n",
    "\n",
    "Many analytics operations (e.g., SUM, AVG, COUNT) operate on columns.\n",
    "\n",
    "Columnar storage allows for optimized vectorized processing.\n",
    "\n",
    "Improved Query Performance:\n",
    "\n",
    "By avoiding unnecessary data retrieval and leveraging compression, Redshift executes analytic queries faster and more efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831400a4-07a5-401e-89db-fe7c3b51b2e5",
   "metadata": {},
   "source": [
    "# 14.Load a CSV file from S3 into Redshift using the COPY command. Share table schema, command used, and sample output from a query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ae49b-c797-4dc0-aaa2-7c80dfb3b36c",
   "metadata": {},
   "source": [
    "1. Table Schema\n",
    "Let's say we have a CSV file with user data like this:\n",
    "\n",
    "user_id\tfirst_name\tlast_name\temail\tsignup_date\n",
    "1\tAlice\tSmith\talice@example.com\t2023-01-15\n",
    "2\tBob\tJohnson\tbob@example.com\t2023-02-20\n",
    "\n",
    "We create a table in Redshift to store this data:\n",
    "\n",
    "CREATE TABLE users (\n",
    "    user_id INT,\n",
    "    first_name VARCHAR(50),\n",
    "    last_name VARCHAR(50),\n",
    "    email VARCHAR(100),\n",
    "    signup_date DATE\n",
    ");\n",
    "\n",
    "COPY users\n",
    "FROM 's3://mybucket/data/users.csv'\n",
    "IAM_ROLE 'arn:aws:iam::123456789012:role/MyRedshiftRole'\n",
    "CSV\n",
    "IGNOREHEADER 1\n",
    "DATEFORMAT 'auto';\n",
    "\n",
    "SELECT * FROM users;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165749c-7da2-4cca-aa67-e39e3e7153b6",
   "metadata": {},
   "source": [
    "# 15.What is the role of the AWS Glue Data Catalog in Athena? How does schema-on-read work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92d9f3-8980-4e37-8c4f-498512e105a2",
   "metadata": {},
   "source": [
    "Role of AWS Glue Data Catalog in Athena:\n",
    "AWS Glue Data Catalog acts as a central metadata repository for Athena.\n",
    "\n",
    "It stores information about your data sources such as databases, tables, schemas, and partitions.\n",
    "\n",
    "When you run a query in Athena, it refers to the Glue Data Catalog to understand the structure of the data (table schema, column types, location, etc.).\n",
    "\n",
    "Glue Data Catalog enables Athena to query data directly from sources like S3 without moving or transforming the data beforehand.\n",
    "\n",
    "It also integrates with other AWS analytics services, providing a unified metadata layer.\n",
    "\n",
    "How Schema-on-Read Works:\n",
    "Traditional databases use schema-on-write: you define the schema before loading data, and data must conform to it.\n",
    "\n",
    "Athena uses schema-on-read, meaning:\n",
    "\n",
    "You define the schema when you run the query, not before loading the data.\n",
    "\n",
    "Athena reads the raw data stored (e.g., in S3) and applies the schema dynamically at query time.\n",
    "\n",
    "This allows querying of semi-structured or raw data without upfront transformation.\n",
    "\n",
    "Benefits of schema-on-read:\n",
    "\n",
    "Flexibility to query data in its original format.\n",
    "\n",
    "No need to preprocess or restructure data before analysis.\n",
    "\n",
    "Easy to adapt to changing data formats or new data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a635d-1ef6-4893-9e31-222f058b4c46",
   "metadata": {},
   "source": [
    "# 16.Create an Athena table from S3 data using Glue Catalog. Run a query and share the SQL + result screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9ebc9-f002-4c08-8348-5e51fe519a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- Step 1: Prepare Your Data in S3\n",
    "# Assume you have a CSV file stored in S3 at:\n",
    "\n",
    "s3://mybucket/data/sales_data.csv\n",
    "Sample data format (CSV):\n",
    "\n",
    "# order_id\tproduct\tquantity\tprice\torder_date\n",
    "# 101\tWidget A\t5\t20.5\t2023-04-01\n",
    "# 102\tWidget B\t3\t15.0\t2023-04-02\n",
    "\n",
    "# Step 2: Create Glue Crawler to Catalog the Data\n",
    "# Go to AWS Glue Console → Crawlers → Add crawler.\n",
    " -->\n",
    "Specify the S3 path (s3://mybucket/data/).\n",
    "\n",
    "# Choose an existing or create a new database, e.g., sales_db.\n",
    "\n",
    "# Run the crawler; it will scan the data and create a table like sales_data with the schema inferred.\n",
    "\n",
    "# Step 3: Verify the Table in Glue Data Catalog\n",
    "# The Glue Data Catalog now has a database sales_db and table sales_data.\n",
    "\n",
    "# It contains schema info (columns, types).\n",
    "\n",
    "# Step 4: Query the Table in Athena\n",
    "# Open Amazon Athena Console and set the database:\n",
    "\n",
    "USE sales_db;\n",
    "# Run a sample query, for example:\n",
    "\n",
    "\n",
    "SELECT product, SUM(quantity) AS total_quantity, SUM(quantity * price) AS total_revenue\n",
    "FROM sales_data\n",
    "GROUP BY product\n",
    "ORDER BY total_revenue DESC;\n",
    "\n",
    "# Step 5: Capture and Share\n",
    "# Take a screenshot of the Athena query editor showing the SQL above.\n",
    "\n",
    "# Also, capture the query results showing aggregated total quantity and revenue per product.\n",
    "\n",
    "# Example SQL for Submission:\n",
    "\n",
    "USE sales_db;\n",
    "\n",
    "SELECT product, \n",
    "       SUM(quantity) AS total_quantity, \n",
    "       SUM(quantity * price) AS total_revenue\n",
    "FROM sales_data\n",
    "GROUP BY product\n",
    "ORDER BY total_revenue DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a1c9f-89ea-40c0-8562-addd199ae8a2",
   "metadata": {},
   "source": [
    "# 17 Describe how Amazon Quicksight supports business intelligence in a serverless data architecture. What are SPICE and embedded dashboards#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814f546-7041-4b21-b131-cc0d7ff4c0fb",
   "metadata": {},
   "source": [
    "<!-- How Amazon QuickSight Supports Business Intelligence in Serverless Data Architecture\n",
    "Serverless BI Service:\n",
    "QuickSight is a fully managed, serverless business intelligence (BI) service by AWS. This means there’s no infrastructure to manage, scale, or patch — AWS handles all backend resources automatically.\n",
    "\n",
    "Seamless Integration with AWS Data Sources:\n",
    "It connects directly to various AWS data services like Athena, Redshift, S3 (via Athena or Redshift Spectrum), RDS, and others, enabling quick access to data stored in the cloud without moving or copying it.\n",
    "\n",
    "Scalable and Cost-Effective:\n",
    "Because it’s serverless, QuickSight scales automatically to support any number of users, from a few analysts to thousands of readers, and charges are usage-based — you pay only for what you use.\n",
    "\n",
    "Fast, Interactive Analytics:\n",
    "Provides interactive dashboards, visualizations, and ad hoc querying with minimal latency, supporting data-driven decision-making.\n",
    "\n",
    "Secure and Compliant:\n",
    "Integrates with AWS IAM and encryption features, providing secure data access and governance.\n",
    "\n",
    "What is SPICE?\n",
    "SPICE stands for Super-fast, Parallel, In-memory Calculation Engine.\n",
    "\n",
    "It is QuickSight’s in-memory data engine that allows rapid data retrieval and interactive analytics.\n",
    "\n",
    "SPICE imports data from various sources and stores it in a highly optimized columnar format in memory.\n",
    "\n",
    "This accelerates query performance, enabling fast dashboards and visualizations even on large datasets.\n",
    "\n",
    "Users can refresh SPICE datasets on a schedule to keep data up-to-date.\n",
    "\n",
    "What are Embedded Dashboards?\n",
    "Embedded dashboards allow you to integrate QuickSight visuals and reports into your own applications or websites.\n",
    "\n",
    "Using APIs and SDKs, you can securely embed interactive dashboards for your customers or internal users without requiring them to log into QuickSight separately.\n",
    "\n",
    "This enables seamless BI experiences within custom portals or SaaS products.\n",
    "\n",
    "Supports fine-grained access control so different users see only data relevant to them.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c053cd6-df9c-4327-b570-356b1ad9856d",
   "metadata": {},
   "source": [
    "# 18.Connect Quicksight to Athena or Redshift and build a dashboard with at least one calculated field and one filter. Share a screenshot of your final dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e219466-920c-4cab-954a-2bc431566528",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Connect QuickSight to Data Source\n",
    "For Athena:\n",
    "\n",
    "In QuickSight console, click Manage data → New data set.\n",
    "\n",
    "Select Athena as the data source.\n",
    "\n",
    "Provide a data source name and choose the AWS region.\n",
    "\n",
    "Choose the database and table (or write a custom query).\n",
    "\n",
    "Import data or use Direct Query mode.\n",
    "\n",
    "For Redshift:\n",
    "\n",
    "In QuickSight console, click Manage data → New data set.\n",
    "\n",
    "Select Redshift as the data source.\n",
    "\n",
    "Provide Redshift cluster endpoint, database, username, and password.\n",
    "\n",
    "Choose the table or write a custom SQL query.\n",
    "\n",
    "Step 2: Create Dataset and Prepare Data\n",
    "After connecting, QuickSight imports or queries the data.\n",
    "\n",
    "You can modify data types and rename columns if needed.\n",
    "\n",
    "Step 3: Create a Calculated Field\n",
    "In the dataset, click Add calculated field.\n",
    "\n",
    "Example:\n",
    "If you have sales data, create a field like Total Revenue = quantity_sold * price_per_unit.\n",
    "\n",
    "Enter the formula and save.\n",
    "\n",
    "Step 4: Build the Dashboard\n",
    "Click New analysis, choose your dataset.\n",
    "\n",
    "Add visuals like bar charts, tables, or pie charts.\n",
    "\n",
    "Add your calculated field as a metric.\n",
    "\n",
    "Add a filter to the dashboard. For example:\n",
    "\n",
    "Filter by date range.\n",
    "\n",
    "Filter by category or region.\n",
    "\n",
    "Step 5: Save and Share\n",
    "Save your analysis.\n",
    "\n",
    "Take a screenshot of the dashboard showing:\n",
    "\n",
    "At least one visual using the calculated field.\n",
    "\n",
    "The filter control panel visible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6697211-e336-45ac-9bb9-384a77953759",
   "metadata": {},
   "source": [
    "# 19.Explain how AWS CloudWatch and CloudTrail differ. IN a data analytics pipeline, what role does each play in monitoring, auditing, and troubleshooting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66dbc7-8267-48e9-ae1c-b1668f9bfc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS CloudWatch -->\n",
    "# Purpose:\n",
    "# CloudWatch is primarily a monitoring and observability service.\n",
    "\n",
    "# What it does:\n",
    "\n",
    "# Collects and tracks metrics (CPU usage, memory, disk I/O, network traffic) from AWS resources and applications.\n",
    "\n",
    "# Captures logs from services like Lambda, EC2, Redshift, etc.\n",
    "\n",
    "# Allows setting alarms to notify you about performance or operational issues.\n",
    "\n",
    "# Provides dashboards and visualizations for real-time monitoring.\n",
    "\n",
    "# Role in Data Analytics Pipeline:\n",
    "\n",
    "# Monitors health and performance of data processing components (e.g., Redshift clusters, Glue jobs, Lambda functions).\n",
    "\n",
    "# Tracks resource utilization to optimize capacity and cost.\n",
    "\n",
    "# Helps troubleshoot failures or bottlenecks by analyzing logs and metrics.\n",
    "\n",
    "# AWS CloudTrail\n",
    "# Purpose:\n",
    "# CloudTrail is a governance, compliance, and auditing service.\n",
    "\n",
    "# What it does:\n",
    "\n",
    "# Records API calls and user activity across your AWS account.\n",
    "\n",
    "# Tracks who did what, when, and from where.\n",
    "\n",
    "# Stores logs of management and data plane events for security auditing and forensic analysis.\n",
    "\n",
    "# Role in Data Analytics Pipeline:\n",
    "\n",
    "# Audits user and system actions on data resources (e.g., who created or modified a Glue job, queried Athena, or accessed S3 buckets).\n",
    "\n",
    "# Ensures compliance with security policies by tracking unauthorized or unusual activities.\n",
    "\n",
    "# Provides audit trails necessary for regulatory requirements. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efb188-e8ee-49a8-9a9c-193d918a92a6",
   "metadata": {},
   "source": [
    "# 20.Describe a complete end-to-end data analytics pipeline using AWS services. Include services for data ingestion, storage, transformation, querying, and visualization. (Example: S3 → Lambda → Glue → Quicksight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7389be-b1c6-4102-bb20-c16c0b886b41",
   "metadata": {},
   "source": [
    "Example Pipeline:\n",
    "Data Ingestion → Storage → Transformation → Querying → Visualization\n",
    "\n",
    "1. Data Ingestion: AWS Kinesis or AWS Lambda\n",
    "AWS Kinesis Data Streams\n",
    "For real-time streaming data ingestion from sources such as IoT devices, application logs, or clickstreams. Kinesis collects and buffers streaming data continuously.\n",
    "\n",
    "AWS Lambda\n",
    "For serverless event-driven ingestion, e.g., processing uploaded files in S3 or API events.\n",
    "\n",
    "2. Data Storage: Amazon S3\n",
    "Amazon S3 serves as a central data lake storing raw and processed data.\n",
    "\n",
    "S3 is durable, scalable, and cost-effective, perfect for storing large volumes of structured and unstructured data.\n",
    "\n",
    "Raw data ingested from Kinesis or Lambda is saved here.\n",
    "\n",
    "3. Data Transformation: AWS Glue\n",
    "AWS Glue performs serverless ETL (Extract, Transform, Load).\n",
    "\n",
    "Glue Crawlers scan the raw data in S3, automatically infer schemas, and populate the Glue Data Catalog.\n",
    "\n",
    "Glue ETL jobs clean, enrich, and transform the raw data into a structured format optimized for analytics.\n",
    "\n",
    "Transformed data is saved back into S3 in a columnar format like Parquet for efficient querying.\n",
    "\n",
    "4. Querying: Amazon Athena or Amazon Redshift\n",
    "Amazon Athena\n",
    "Serverless, interactive SQL querying directly on data stored in S3 using the Glue Data Catalog metadata. Ideal for ad hoc queries without needing to move data.\n",
    "\n",
    "Amazon Redshift\n",
    "For high-performance data warehousing and complex analytics, where data can be loaded from S3 or queried using Redshift Spectrum directly on S3 data.\n",
    "\n",
    "5. Visualization: Amazon QuickSight\n",
    "Amazon QuickSight connects to Athena, Redshift, or other data sources to create interactive dashboards and visualizations.\n",
    "\n",
    "It enables business users and analysts to explore data, generate insights, and share reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc1ee4-3240-47e1-b316-ab77db05bfe8",
   "metadata": {},
   "source": [
    "# 21.Explain why you would choose each service for the stage it’s used in4\n",
    "# _ Submission Instructions Format\n",
    "# One .docx or .pdf file containing\n",
    "# _ Written responses to theory question<\n",
    "# _ Screenshots and code snippets for hands-on task<\n",
    "# _ Clear question numbers and titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb89e2-e303-4405-a645-6238d09fd0ae",
   "metadata": {},
   "source": [
    "When designing a data workflow or pipeline, selecting the right AWS service for each stage depends on the specific requirements of data ingestion, storage, processing, and analysis. Here’s a typical breakdown:\n",
    "\n",
    "1. Data Ingestion: AWS Kinesis / AWS Glue / AWS Data Pipeline\n",
    "Why choose?\n",
    "\n",
    "AWS Kinesis: Real-time streaming data ingestion, low latency.\n",
    "\n",
    "AWS Glue: Serverless ETL to crawl and catalog data for batch ingestion.\n",
    "\n",
    "AWS Data Pipeline: Orchestrate and automate data movement and transformation on schedules.\n",
    "\n",
    "2. Data Storage: Amazon S3 / Amazon Redshift / Amazon RDS\n",
    "Why choose?\n",
    "\n",
    "Amazon S3: Cost-effective, durable, scalable object storage for raw or processed data lakes.\n",
    "\n",
    "Amazon Redshift: Fully managed, high-performance data warehouse optimized for complex analytical queries.\n",
    "\n",
    "Amazon RDS: Managed relational database for transactional workloads with structured data.\n",
    "\n",
    "3. Data Processing & Transformation: AWS Glue / AWS EMR / AWS Lambda\n",
    "Why choose?\n",
    "\n",
    "AWS Glue: Serverless ETL with built-in data catalog integration, ideal for batch transformations.\n",
    "\n",
    "AWS EMR: Managed Hadoop/Spark clusters for big data processing, custom analytics workloads.\n",
    "\n",
    "AWS Lambda: Event-driven serverless functions for lightweight, real-time data processing.\n",
    "\n",
    "4. Data Analytics & Visualization: Amazon Athena / Amazon QuickSight / Amazon Redshift Spectrum\n",
    "Why choose?\n",
    "\n",
    "Amazon Athena: Serverless interactive querying directly on data in S3 using SQL, great for ad hoc analysis.\n",
    "\n",
    "Amazon QuickSight: Scalable business intelligence tool for creating visual dashboards and reports.\n",
    "\n",
    "Redshift Spectrum: Extends Redshift to query data directly in S3 without loading it, combining warehouse and data lake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85769920-5406-4c12-a536-8377297cfcb2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
